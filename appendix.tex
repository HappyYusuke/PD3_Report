%ここからソースコードの表示に関する設定
\lstset{
  basicstyle={\ttfamily},
  identifierstyle={\small},
  commentstyle={\smallitshape},
  keywordstyle={\small\bfseries},
  ndkeywordstyle={\small},
  stringstyle={\small\ttfamily},
  frame={tb},
  breaklines=true,
  columns=[l]{fullflexible},
  numbers=left,
  xrightmargin=0zw,
  xleftmargin=3zw,
  numberstyle={\scriptsize},
  stepnumber=1,
  numbersep=1zw,
  lineskip=-0.5ex
}
%ここまでソースコードの表示に関する設定
パラメータの設定ファイルを、ソースコード\ref{param}に示す。
\begin{lstlisting}[caption=follow\_me\_params.yaml, label=param]
/follow_me/laser_to_img:
  ros__parameters:
    # 縮小サイズを取得. 1[px] = 0.01[m]
    discrete_size: 0.01
    # Max LiDAR Range
    max_lidar_range: 3.5
    # 画像を表示するフラッグ
    img_show_flg: False

/follow_me/person_detector:
  ros__parameters:
    # 追従対象者との距離
    target_dist: 0.5
    # 追従対象を見失ったときに追従を再開する時の距離の誤差
    target_diff: 0.3
    # 追従ポイント（制御を止める領域）の半径
    target_radius: 0.1
    # 人を検出する範囲(円)の半径
    target_range: 0.4
    # 起動時に追従対象者を検出するまでの待機時間
    init_time: 3.0
    # 起動時に追従対象を検出するまでのflg
    none_person_flg: True
  
/follow_me/base_controller:
  ros__parameters:
    # ロボットからみてtolerance[°]以内だったら積分制御しない視野角
    tolerance: 1.0
    # 積分制御をし始める視野角[°]
    i_range: 3.0
    # 並進のPゲイン==========================
    lkp: 0.3
    # 旋回のPIDゲイン========================
    # Pゲイン
    akp: 0.005
    # Iゲイン
    aki: 0.0
    # Dゲイン
    akd: 0.0009
\end{lstlisting}

2D-LiDARの距離データから俯瞰画像を生成するソースコードを、ソースコード\ref{image}に示す。
\begin{lstlisting}[caption=laser\_to\_image.py, label=image]
import numpy as np
import os
import sys
import cv2
import math
import rclpy
from rclpy.node import Node
from rclpy.parameter import Parameter
from sensor_msgs.msg import LaserScan, Image
from rclpy.qos import qos_profile_sensor_data
from rcl_interfaces.msg import SetParametersResult
from cv_bridge import CvBridge, CvBridgeError
# Custom
from .modules.gradient import gradation_3d_img as gradation


class LaserToImg(Node):
    def __init__(self):
        super().__init__('laser_to_img')
        # Publisher
        self.pub = self.create_publisher(Image, '/follow_me/laser_img', 10)
        # Subscriber
        self.create_subscription(LaserScan, '/scan', self.cloud_to_img_callback, qos_profile_sensor_data)
        # OpenCV
        self.bridge = CvBridge()
        # Parameters
        self.declare_parameters(
                namespace='',
                parameters=[
                    ('discrete_size', Parameter.Type.DOUBLE),
                    ('max_lidar_range', Parameter.Type.DOUBLE),
                    ('img_show_flg', Parameter.Type.BOOL)])
        self.add_on_set_parameters_callback(self.param_callback)
        # Get parameters
        self.param_dict = {}
        self.param_dict['discrete_size'] = self.get_parameter('discrete_size').value
        self.param_dict['max_lidar_range'] = self.get_parameter('max_lidar_range').value
        self.param_dict['img_show_flg'] = self.get_parameter('img_show_flg').value
        # Values
        self.color_list = gradation([0,0,255], [255,0,0], [1, 100], [True,True,True])[0]
        # Output
        self.output_screen()

    def output_screen(self):
        for key, value in self.param_dict.items():
            self.get_logger().info(f"{key}: {value}")

    def param_callback(self, params):
        for param in params:
            self.param_dict[param.name] = param.value
            self.get_logger().info(f"Set param: {param.name} >>> {param.value}")
        return SetParametersResult(successful=True)

    def cloud_to_img_callback(self, scan):
        
        # discrete_factor
        discrete_factor = 1/self.param_dict['discrete_size']
        # max_lidar_rangeとdiscrete_factorを使って画像サイズを設定する
        img_size = int(self.param_dict['max_lidar_range']*2*discrete_factor)

        # LiDARデータ
        maxAngle = scan.angle_max
        minAngle = scan.angle_min
        angleInc = scan.angle_increment
        maxLength = scan.range_max
        ranges = scan.ranges
        intensities = scan.intensities
        #intensities = scan.intensities
        
        # 距離データの個数を格納
        num_pts = len(ranges)
        # 721行2列の空行列を作成
        xy_scan = np.zeros((num_pts, 2))
        # 3チャンネルの白色ブランク画像を作成
        blank_img = np.zeros((img_size, img_size, 3), dtype=np.uint8) + 255
        # rangesの距離・角度からすべての点をXYに変換する処理
        for i in range(num_pts):
            # 範囲内かを判定
            if (ranges[i] > self.param_dict['max_lidar_range']) or (math.isnan(ranges[i])):
                pass
            else:
                # 角度とXY座標の算出処理
                angle = minAngle + float(i)*angleInc
                xy_scan[i][1] = float(ranges[i]*math.cos(angle))  # y座標
                xy_scan[i][0] = float(ranges[i]*math.sin(angle))  # x座標

        # ブランク画像にプロットする処理
        for i in range(num_pts):
            pt_x = xy_scan[i, 0]
            pt_y = xy_scan[i, 1]
            if (pt_x < self.param_dict['max_lidar_range']) or (pt_x > -1*(self.param_dict['max_lidar_range']-self.param_dict['discrete_size'])) or (pt_y < self.param_dict['max_lidar_range']) or (pt_y > -1 * (self.param_dict['max_lidar_range']-self.param_dict['discrete_size'])):
                pix_x = int(math.floor((pt_x + self.param_dict['max_lidar_range']) * discrete_factor))
                pix_y = int(math.floor((self.param_dict['max_lidar_range'] - pt_y) * discrete_factor))
                if (pix_x > img_size) or (pix_y > img_size):
                    print("Error")
                else:
                    blank_img[pix_y, pix_x] = [0, 0, 0]

        # CV2画像からROSメッセージに変換してトピックとして配布する
        img = self.bridge.cv2_to_imgmsg(blank_img, encoding="bgr8")
        self.pub.publish(img)

        # 画像の表示処理. imgshow_flgがTrueの場合のみ表示する
        if self.param_dict['img_show_flg']:
            cv2.imshow('laser_img', blank_img)
            cv2.waitKey(3)
            #更新のため一旦消す
            blank_img = np.zeros((img_size, img_size, 3))
        else:
            pass

def main():
    rclpy.init()
    node = LaserToImg()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    node.destroy_node()
    rclpy.shutdown()
\end{lstlisting}

追従対象者を特定するソースコードを、ソースコード\ref{person}に示す。
\begin{lstlisting}[caption=person\_detector.py, label=person]
import math
import time
import cv2
import rclpy
from rclpy.node import Node
from rclpy.parameter import Parameter
from rcl_interfaces.msg import SetParametersResult, ParameterEvent
from rcl_interfaces.srv import GetParameters
from sensor_msgs.msg import Image
from geometry_msgs.msg import Point
from cv_bridge import CvBridge, CvBridgeError
from yolov8_msgs.msg import DetectionArray


class PersonDetector(Node):
    def __init__(self):
        super().__init__('person_detector')
        # OpenCV Bridge
        self.bridge = CvBridge()
        # Publisher
        self.point_pub = self.create_publisher(Point, '/follow_me/target_point', 10)
        self.img_pub = self.create_publisher(Image, '/follow_me/image', 10)
        # Subscriber
        self.create_subscription(DetectionArray, '/yolo/detections', self.yolo_callback, 10)
        self.create_subscription(Image, '/yolo/dbg_image', self.img_show, 10)
        self.create_subscription(ParameterEvent, '/parameter_events', self.param_event_callback, 10)
        # Service
        self.srv_client = self.create_client(GetParameters, '/follow_me/laser_to_img/get_parameters')
        while not self.srv_client.wait_for_service(timeout_sec=0.5):
            self.get_logger().info('/follow_me/laser_to_img server is not available ...')
        # Parameters
        self.declare_parameters(
                namespace='',
                parameters=[
                    ('target_dist', Parameter.Type.DOUBLE),
                    ('init_time', Parameter.Type.DOUBLE),
                    ('none_person_flg', Parameter.Type.BOOL),
                    ('target_diff', Parameter.Type.DOUBLE),
                    ('target_radius', Parameter.Type.DOUBLE),
                    ('target_range', Parameter.Type.DOUBLE)])
        self.add_on_set_parameters_callback(self.param_callback)
        # Get parameters
        self.param_dict ={}
        self.param_dict['target_dist'] = self.get_parameter('target_dist').value
        self.param_dict['init_time'] = self.get_parameter('init_time').value
        self.param_dict['none_person_flg'] = self.get_parameter('none_person_flg').value
        self.param_dict['target_diff'] = self.get_parameter('target_diff').value
        self.param_dict['target_radius'] = self.get_parameter('target_radius').value
        self.param_dict['target_range'] = self.get_parameter('target_range').value
        self.param_dict['discrete_size'] = self.get_param()  # laser_to_imgからもってくる
        # Value
        self.person_list = []
        self.target_data = []  # 追従対象のデータを保存するリスト
        self.before_data = [0.0, 0.0, 0.0]
        self.center_x = 0.0
        self.center_y = 0.0
        self.target_point = Point()
        self.target_px = []
        self.laser_img = 0.0
        self.height = 0.0
        self.width = 0.0
        # Output
        self.output_screen()

    def output_screen(self):
        for key, value in self.param_dict.items():
            self.get_logger().info(f"{key}: {value}")

    def param_event_callback(self, receive_msg):
        for data in receive_msg.changed_parameters:
            if data.name == 'discrete_size':
                self.param_dict['discrete_size'] = data.value.double_value
                self.get_logger().info(f"Param event: {data.name} >>> {self.param_dict['discrete_size']}")

    def param_callback(self, params):
        for param in params:
            self.param_dict[param.name] = param.value
            self.get_logger().info(f"Set param: {param.name} >>> {param.value}")
        return SetParametersResult(successful=True)
    
    def get_param(self):
        req = GetParameters.Request()
        req.names = ['discrete_size']
        future = self.srv_client.call_async(req)
        while rclpy.ok():
            rclpy.spin_once(self, timeout_sec=0.1)
            if future.done():
                break
        return future.result().values[0].double_value

    def yolo_callback(self, receive_msg):
        if not receive_msg.detections:
            self.center_x = self.center_y = None
        else:
            self.person_list.clear()
            for person in receive_msg.detections:
                px = Point()
                px.x = person.bbox.center.position.x
                px.y = person.bbox.center.position.y
                self.person_list.append(px)

    def plot_robot_point(self):
        # 画像の中心を算出
        robot_x = round(self.width / 2)
        robot_y = round(self.height / 2)
        # 描画処理
        cv2.circle(img = self.laser_img,
                   center = (round(robot_x), round(robot_y)),
                   radius = 5,
                   color = (0, 255, 0),
                   thickness = -1)

    def plot_person_point(self):
        cv2.circle(img = self.laser_img,
                   center = (round(self.center_x), round(self.center_y)),
                   radius = 8,
                   color = (0, 0, 255),
                   thickness = -1)

    def plot_target_point(self):
        cv2.circle(img = self.laser_img,
                   center = (int(self.target_px[0]), int(self.target_px[1])),
                   radius = int(self.param_dict['target_radius']/self.param_dict['discrete_size']),
                   color = (255, 0, 0),
                   thickness = 2)

    def plot_target_range(self):
        cv2.circle(img = self.laser_img,
                   center = (int(self.before_data[2][0]), int(self.before_data[2][1])),
                   radius = int(self.param_dict['target_range']/self.param_dict['discrete_size']),
                   color = (196, 0, 255),
                   thickness = 2)

    def diff_distance(self, data):
        return abs(data - self.before_data[0])

    def euclidean_distance(self, data, before_data):
        return math.sqrt((data.x-before_data.x)**2 + (data.y-before_data.y)**2)

    def select_target(self, robot_px_x, robot_px_y):
        # personまでの距離と座標のリストを作成
        self.target_data.clear()
        for person_px in self.person_list:
            person_point = Point()
            person_point.x = (robot_px_x - person_px.y)*self.param_dict['discrete_size']
            person_point.y = (robot_px_y - person_px.x)*self.param_dict['discrete_size']
            distance = math.sqrt(person_point.x**2 + person_point.y**2)
            self.target_data.append([distance, person_point, [person_px.x, person_px.y]])
        # 0番目に1時刻前の追従対象との距離の誤差を格納する
        self.target_data = [[self.diff_distance(data[0]), data[1], data[2]] for data in self.target_data]
        # 検出範囲内のpersonを追従対象とする(起動時だけ一番近い人を追従対象にする)
        if self.param_dict['none_person_flg']:
            target = min(self.target_data)
            param_bool = Parameter('none_person_flg', Parameter.Type.BOOL, False)
            self.set_parameters([param_bool])
        else:
            for data in self.target_data:
                diff = self.euclidean_distance(data[1], self.before_data[1])
                if diff <= self.param_dict['target_range']:
                    target = data
                    break
                else:
                    target = None
        # targetがNoneだったらself.before_dataを初期化してnone_person_flgをTrueにする
        if target is None:
            #self.before_data = [0.0, 0.0, 0.0]
            #param_bool = Parameter('none_person_flg', Parameter.Type.BOOL, True)
            #self.set_parameters([param_bool])
            pass
        else:
            # 計算のために距離を保存
            self.before_data = target
        return target

    def generate_target(self):
        self.target_px.clear()
        # 画像の中心を算出
        robot_x = self.height / 2
        robot_y = self.width / 2
        # 追従目標を選定
        target_person = self.select_target(robot_x, robot_y)
        if not target_person is None:
            result_point = target_person[1]
            self.center_x = target_person[2][0]
            self.center_y = target_person[2][1]
        else:
            result_point = False
        return result_point


    def img_show(self, receive_msg):
        self.laser_img = self.bridge.imgmsg_to_cv2(receive_msg, desired_encoding='bgr8')
        self.height, self.width, _ = self.laser_img.shape[:3]
        # ロボットの座標をプロット
        self.plot_robot_point()
        # 追従対象の検出範囲をプロット
        if not self.param_dict['none_person_flg']:
            self.plot_target_range()
        # personがいるか判定
        if self.person_list:
            # 追従対象を生成
            target_point = self.generate_target()
            # 追従対象がいなければロボット台車を停止する
            if not target_point:
                self.target_point.x = 0.0
                self.target_point.y = 0.0
                self.point_pub.publish(self.target_point)
            else:
                robot_x = self.height / 2
                robot_y = self.width / 2
                # 目標座標を生成(px): 横x, 縦y
                target_x = self.center_x
                target_y = self.center_y + (self.param_dict['target_dist']/self.param_dict['discrete_size'])
                self.target_px.append(target_x)
                self.target_px.append(target_y)
                # 目標座標を生成(m): 縦x, 横y(ロボット座標系に合わせる)
                self.target_point.x = (robot_x - target_y)*self.param_dict['discrete_size']
                self.target_point.y = (robot_y - target_x)*self.param_dict['discrete_size']
                # パブリッシュ
                self.point_pub.publish(self.target_point)
                # グラフに描画
                self.plot_target_point()
                self.plot_person_point()

        # ros2 bag 用にトピックとして画像を配布
        img = self.bridge.cv2_to_imgmsg(self.laser_img, encoding="bgr8")
        self.img_pub.publish(img)

        # 画像を表示
        cv2.imshow('follow_me', self.laser_img)
        cv2.waitKey(1)


def main():
    rclpy.init()
    node = PersonDetector()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    node.destroy_node()
    rclpy.shutdown()
\end{lstlisting}

ロボット台車を制御するソースコードを、ソースコード\ref{base}に示す。
\begin{lstlisting}[caption=base\_controller.py, label=base]
import time
import math
import rclpy
from rclpy.node import Node
from rclpy.parameter import Parameter
from rcl_interfaces.msg import SetParametersResult, ParameterEvent
from rcl_interfaces.srv import GetParameters
from nav_msgs.msg import Odometry
from geometry_msgs.msg import Twist, Point


class BaseController(Node):
    def __init__(self):
        super().__init__('base_controller')
        # Publisher
        self.pub = self.create_publisher(Twist, '/cmd_vel', 10)
        self.data_pub = self.create_publisher(Point, '/follow_me/distance_angle_data', 10)
        # Subscriber
        self.create_subscription(Point, '/follow_me/target_point', self.callback, 10)
        self.create_subscription(Odometry, '/odom', self.odom_callback, 10)
        self.create_subscription(ParameterEvent, '/parameter_events', self.param_event_callback, 10)
        # Service
        self.srv_client = self.create_client(GetParameters, '/follow_me/person_detector/get_parameters')
        while not self.srv_client.wait_for_service(timeout_sec=0.5):
            self.get_logger().info('/follow_me/laser_to_img server is not available ...')
        # Parameters
        self.declare_parameters(
                namespace='',
                parameters=[
                    ('tolerance', Parameter.Type.DOUBLE),
                    ('i_range', Parameter.Type.DOUBLE),
                    ('lkp', Parameter.Type.DOUBLE),
                    ('akp', Parameter.Type.DOUBLE),
                    ('aki', Parameter.Type.DOUBLE),
                    ('akd', Parameter.Type.DOUBLE)])
        self.add_on_set_parameters_callback(self.param_callback)
        # Get parameters
        self.param_dict = {}
        self.param_dict['tolerance'] = self.get_parameter('tolerance').value
        self.param_dict['i_range'] = self.get_parameter('i_range').value
        self.param_dict['lkp'] = self.get_parameter('lkp').value
        self.param_dict['akp'] = self.get_parameter('akp').value
        self.param_dict['aki'] = self.get_parameter('aki').value
        self.param_dict['akd'] = self.get_parameter('akd').value
        self.param_dict['target_radius'] = self.get_param()  # person_detectorからもってくる
        # Value
        self.twist = Twist()
        self.target_angle = 0.0
        self.target_distance = 0.0
        self.target_x = 0.0
        self.target_y = 0.0
        self.delta_t = 0.0
        self.robot_angular_vel = 0.0
        # Output
        self.output_screen()

    def output_screen(self):
        for key, value in self.param_dict.items():
            self.get_logger().info(f"{key}: {value}")

    def param_event_callback(self, receive_msg):
        for data in receive_msg.changed_parameters:
            if data.name == 'target_radius':
                self.param_dict['target_radius'] = data.value.double_value
                self.get_logger().info(f"Param event: {data.name} >>> {self.param_dict['target_radius']}")

    def param_callback(self, params):
        for param in params:
            self.param_dict[param.name] = param.value
            self.get_logger().info(f"Set param: {param.name} >>> {param.value}")
        return SetParametersResult(successful=True)

    def get_param(self):
        req = GetParameters.Request()
        req.names = ['target_radius']
        future = self.srv_client.call_async(req)
        while rclpy.ok():
            rclpy.spin_once(self, timeout_sec=0.1)
            if future.done():
                break
        return future.result().values[0].double_value

    def point_to_angle(self, point):
        return math.degrees(math.atan2(point.y, point.x))

    def point_to_distance(self, point):
        distance = math.sqrt(point.x**2 + point.y**2)
        if point.x < 0.0:
            distance = -distance
        return distance

    def callback(self, receive_msg):
        self.target_x = receive_msg.x
        self.target_y = receive_msg.y
        self.target_distance = self.point_to_distance(receive_msg)
        self.target_angle = self.point_to_angle(receive_msg)

    def odom_callback(self, receive_msg):
        self.robot_angular_vel = receive_msg.twist.twist.angular.z

    # 比例制御量計算
    def p_control(self):
        return self.param_dict['akp']*self.target_angle

    # 微分制御量計算
    def d_control(self, p_term):
        return self.param_dict['akd']*(p_term - self.robot_angular_vel)

    # 積分制御量計算
    def i_control(self, p_term, d_term):
        value = 0.0
        diff = (p_term + d_term) - self.robot_angular_vel

        if not diff < self.param_dict['tolerance'] and diff < self.param_dict['i_range']:
            value = self.param_dict['aki']*self.target_angle*self.delta_t

        return value

    def pid_update(self):
        # 制御量を計算
        p_term = self.p_control()
        d_term = self.d_control(p_term)
        i_term = self.i_control(p_term, d_term)

        linear_vel = self.param_dict['lkp']*self.target_distance
        angular_vel = -1*(p_term + i_term + d_term)

        if linear_vel < 0.0:
            linear_vel = 0.0
            angular_vel = 0.0

        return linear_vel, angular_vel

    def in_range(self):
        result = False
        if abs(self.target_distance) <= self.param_dict['target_radius']:
            result = True
        return result

    def execute(self, rate=100):
        start_time = time.time()
        before_time = 0.0

        while rclpy.ok():
            self.delta_t = time.time() - start_time
            rclpy.spin_once(self)

            # 許容範囲内外を判
            if self.in_range():
                linear_vel = 0.0
                angular_vel = 0.0
            else:
                linear_vel, angular_vel = self.pid_update()

            # 制御量をパブリッシュ
            self.twist.linear.x = linear_vel
            self.twist.angular.z = angular_vel
            self.pub.publish(self.twist)

            # 実験用に目標までの距離と角度をパブリッシュ
            data = Point()
            data.x = self.target_distance
            data.z = self.target_angle
            self.data_pub.publish(data)

            time.sleep(1/rate)


def main():
    rclpy.init()
    node = BaseController()
    try:
        node.execute()
    except KeyboardInterrupt:
        pass

    node.destroy_node()
    rclpy.shutdown()
\end{lstlisting}

以上のソースコードをまとめて起動するソースコードを、ソースコード\ref{launch}に示す。
\begin{lstlisting}[caption=follow\_me.launch.py, label=launch]
import os
from ament_index_python.packages import get_package_share_directory
import launch
from launch import LaunchDescription
from launch.actions import DeclareLaunchArgument
from launch_ros.actions import Node

def generate_launch_description():
    config = os.path.join(
        get_package_share_directory('recognition_by_lidar'),
        'config',
        'follow_me_params.yaml')

    namespace = 'follow_me'

    return LaunchDescription([
        Node(
            namespace=namespace,
            package='recognition_by_lidar',
            executable='laser_to_img',
            name='laser_to_img',
            parameters=[config],
            output='screen',
            respawn=True),
        Node(
            namespace=namespace,
            package='recognition_by_lidar',
            executable='person_detector',
            name='person_detector',
            parameters=[config],
            output='screen',
            respawn=True),
        Node(
            namespace=namespace,
            package='recognition_by_lidar',
            executable='base_controller',
            name='base_controller',
            parameters=[config],
            output='screen',
            respawn=True,
            on_exit=launch.actions.Shutdown()),
        ])
\end{lstlisting}